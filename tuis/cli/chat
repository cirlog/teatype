#!/usr/bin/env python3.11

# Copyright (C) 2024-2026 Burak GÃ¼naydin
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.

# Third-party imports
from teatype import colorwrap
from teatype.ai.models.llm import __GPU_SUPPORT__
from teatype.cli import BaseTUI
from teatype.io import fetch, path, prompt
from teatype.logging import *

if __GPU_SUPPORT__:
    from teatype.ai.models.llm import ConversationalAI

DEFAULT_MODEL_PATH = ''
MANUAL_REFRESH = False # If True, the shell will be cleared automatically after each command
DEBUG_MODE = False

# FIXME: Missmatched parent paths
class Chat(BaseTUI):
    NOT_AVAILABLE=not __GPU_SUPPORT__
    NOT_AVAILABLE_REASON='no gpu-support'
    
    def meta(self):
        return {
            'name': 'chat',
            'shorthand': 'ch',
            'help': 'Chat with a local LLM model',
        }
        
    def on_display(self):
        pass
    
    def on_prompt(self):
        pass

    def run(self):
        println()
        
        parent_directory = path.caller_parent(reverse_depth=2)
        cli_dist_directory = path.join(parent_directory, 'dist')
        model_directory = path.join(cli_dist_directory, 'llm-models')
        conversational_model_directory = path.join(model_directory, 'conversational')
        if not path.exists(conversational_model_directory):
            warn(f'Conversational model directory not found at {conversational_model_directory}. Creating it. Please re-run this script after placing your model there.',
                 use_prefix=False)
            path.create(conversational_model_directory)
            println()
            return
        
        stream = self.get_flag('stream')
        if not stream:
            stream = True
        # def _spinner(stop_event):
        #     for symbol in itertools.cycle('|/-\\'):
        #         if stop_event.is_set():
        #             break
        #         sys.stdout.write('\rThinking ' + symbol)
        #         sys.stdout.flush()
        #         time.sleep(0.1)
        #     sys.stdout.write('\r' + ' ' * 20 + '\r') # clear line
        # stop_event = threading.Event()
        # spinner_thread = threading.Thread(target=_spinner, args=(stop_event,))
        # spinner_thread.start()
        
        # llm_client = LaunchPad.create(LLMClient,
        #                               auto_fire=True,
        #                               verbose_logging=verbose)
        # llm_client.load_model(model_path=path.join(conversational_model_directory, f'{default_model}.gguf'))
        # llm_client.chat(message=self.full_prompt)
        
        # while True:
        #     time.sleep(0.01)
        #     if llm_client.first_token.is_set():
        #         stop_event.set()
        #         spinner_thread.join()
        #     if not llm_client.receiving_response.is_set():
        #         break
        
        # println()
        # println()
            
        llm = ConversationalAI(model='Nous-Hermes-2-Mistral-7B-DPO.Q6_K',
                               model_directory=conversational_model_directory,
                               temperature=0.9,
                               top_p=0.9,
                               verbose=True)
        println()
        log(colorwrap('[LLM]:', 'cyan'))
        response = llm.chat('Instruction: Greet the user.', stream_response=stream)
        try:
            while True:
                user_input = prompt('[You]:', return_bool=False)
                if user_input.lower() in ['exit', 'quit', 'q', 'bye']:
                    println()
                    break
                println()
                log(colorwrap('[LLM]:', 'cyan'))
                response = llm.chat(user_input, stream_response=stream)
                if not stream:
                    print(response)
        except KeyboardInterrupt:
            println()
        
if __name__ == '__main__':
    Chat()