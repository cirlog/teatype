#!/usr/bin/env python3.11

# Copyright (C) 2024-2025 Burak GÃ¼naydin
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.

# Third-party imports
from teatype import colorwrap
from teatype.ai import __GPU_SUPPORT__
from teatype.cli import BaseCLI
from teatype.io import fetch, file, path, prompt
from teatype.logging import *
from teatype.modulo import LaunchPad, Operations

if __GPU_SUPPORT__:
    from teatype.ai.llm import LLMEngine

# FIXME: Missmatched parent paths
class Ask(BaseCLI):
    NOT_AVAILABLE=not __GPU_SUPPORT__
    NOT_AVAILABLE_REASON='no gpu-support'
    
    def meta(self):
        return {
            'name': 'ask',
            'shorthand': 'ak',
            'help': 'Ask a one-shot question to a local LLM model',
            'arguments': [
                {
                    'name': 'question',
                    'help': 'The question to ask the model',
                    'required': True
                }
            ],
            'flags': [
                {
                    'short': 'lm',
                    'long': 'load-model',
                    'help': 'Load and save a default model for future use',
                    'required': False
                },
                {
                    'short': 's',
                    'long': 'stream',
                    'help': 'Stream the response as it is generated',
                    'required': False  
                },
                {
                    'short': 'v',
                    'long': 'verbose',
                    'help': 'Enable verbose output',
                    'required': False
                }
            ]
        }
        
    def pre_validate(self):
        try:
            import llama_cpp
        except ImportError:
            err('teatype was not installed with gpu-support.',
                exit=True,
                pad_after=1,
                pad_before=1)
        
    def post_validate(self):
        self.full_prompt = ' '.join(self.parsed_arguments)
        self._parsing_errors = []
        
    def execute(self):
        println()
        
        verbose = self.get_flag('verbose')
        
        parent_directory = path.caller_parent(reverse_depth=2)
        cli_dist_directory = path.join(parent_directory, 'dist')
        model_directory = path.join(cli_dist_directory, 'llm-models')
        conversational_model_directory = path.join(model_directory, 'conversational')
        if not path.exists(conversational_model_directory):
            warn(f'Conversational model directory not found at {conversational_model_directory}. Creating it. Please re-run this script after placing your model there.',
                 use_prefix=False)
            path.create(conversational_model_directory)
            println()
            return

        stream = self.get_flag('stream')
        if not stream:
            stream = True
        
        default_model_file_path = path.join(conversational_model_directory, 'default-model.txt')
        if not file.exists(default_model_file_path):
            file.write(default_model_file_path, '')
        
        default_model = file.read(default_model_file_path).strip()
        if default_model == '' or default_model is None or self.get_flag('load-model'):
            hint('No default model set. Select one of these available local models:',
                 use_prefix=False)
            available_local_models = [f.name.split('.gguf')[0] for f in file.list(conversational_model_directory, only_include='.gguf')]
            prompt_options = {str(i+1): model_name for i, model_name in enumerate(available_local_models)}
            for available_model_index, available_model_name in prompt_options.items():
                model_file_path = path.join(conversational_model_directory, f'{available_model_name}.gguf')
                file_size = file.size(model_file_path, human_readable=True)
                log(f'  [{available_model_index}] {available_model_name} ({file_size})')
            selection = prompt('Please enter the number corresponding to your choice:',
                               options=prompt_options,
                               return_bool=False)
            default_model = available_local_models[int(selection)-1]
            file.write(default_model_file_path, default_model)
        
        default_model = file.read(path.join(conversational_model_directory, 'default-model.txt'))
        
        operations = Operations(verbose_logging=False if verbose == None else verbose)
        units = operations.list(filters=[('type', 'llm-engine')])
        if len(units) == 0:
            LaunchPad.fire(LLMEngine)
            # llm = Inferencer(model=default_model,
            #                  model_directory=conversational_model_directory)
            
            # response = llm(user_prompt=self.full_prompt,
            #                stream_response=stream)
        else:
            if verbose:
                log('Found existing LLM Engine unit.')
            llm_unit = units[0]
            operations.dispatch(llm_unit.get('id'),
                                command='load-model')
        
        println()
if __name__ == '__main__':
    Ask()